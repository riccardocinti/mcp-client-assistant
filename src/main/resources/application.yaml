spring:
  application:
    name: mcp-client-assistant
  ai:
    ollama:
      timeout: 300s
      chat:
        options:
          base-url: http://localhost:11434
          model: llama3.1:8b
          temperature: 0.8
          max-tokens: 4096
          stream: true
          options:
            num-gpu: 999  # Use all GPU layers
            repeat-penalty: 1.2
    mcp:
      client:
        request-timeout: 60s
        stdio:
          connections:
            github:
              command: java
              args:
                - "-jar"
                - "./servers/mcp-server-github-0.0.1-SNAPSHOT.jar"
            builder:
              command: java
              args:
                - "-jar"
                - "./servers/mcp-server-builder-0.0.1-SNAPSHOT.jar"
        toolcallback:
          enabled: true

# Management Endpoints (Actuator)
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus
      base-path: /actuator
  endpoint:
    health:
      show-details: always
      probes:
        enabled: true
  health:
    livenessstate:
      enabled: true
    readinessstate:
      enabled: true

# Server Configuration
server:
  port: 8080
  error:
    include-message: always
    include-stacktrace: on-param
  # Timeouts for long-running AI operations
  servlet:
    context-path: /
    session:
      timeout: 30m
  tomcat:
    connection-timeout: 300s

# Logging Configuration
logging:
  level:
    root: DEBUG
    com.riccardocinti.mcp_client_assistant.mcp_client_assistant: DEBUG
    org.springframework.ai: DEBUG
    org.springframework.ai.mcp: DEBUG
    org.springframework.ai.ollama: DEBUG
    org.springframework.web: INFO

  # Log format
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss} - %logger{36} - %level - %msg%n"
    file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
